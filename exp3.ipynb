{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01bc4f5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30px; font-weight: bold; margin-bottom: 20px;\">\n",
    "    Program 3\n",
    "</div>\n",
    "\n",
    "\n",
    "### **Aim**\n",
    "Understanding Artificial Neural Networks (ANN) and the role of activation functions in influencing accuracy and data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9450a",
   "metadata": {},
   "source": [
    "### **Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea9370",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron (MLP)\n",
    "\n",
    "A Multilayer Perceptron is a deep neural network composed of fully connected layers, each followed by a non-linear activation function. MLPs are capable of learning complex non-linear patterns through iterative weight updates using backpropagation. They rely on dense connections and activations to transform input data into meaningful internal representations.\n",
    "\n",
    "#### Data Augmentation\n",
    "\n",
    "Data augmentation artificially expands the training dataset by applying random transformations such as rotations, translations, flips, and intensity changes. For image tasks, augmentation increases data variability and helps the model generalize better to unseen samples. It reduces overfitting by preventing the network from memorizing training examples and exposes the model to more diverse scenarios without collecting new data.\n",
    "\n",
    "#### Optimizers\n",
    "\n",
    "Optimizers control how model weights are updated during training.\n",
    "\n",
    "* **SGD (Stochastic Gradient Descent)** performs parameter updates using mini-batch gradients. It is simple and effective but may converge slowly.\n",
    "* **Adam** adapts learning rates for each parameter using estimates of first and second moments of gradients, allowing faster and more stable convergence.\n",
    "* **RMSProp** maintains a moving average of squared gradients and adapts learning rates, making it effective for training non-stationary problems.\n",
    "\n",
    "Different optimizers influence training speed, stability, and final accuracy depending on the dataset and architecture.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "Dropout randomly disables a fraction of neurons during training with a specified probability. This prevents neurons from co-adapting too heavily on specific features, thereby reducing overfitting. When dropout is used, the network learns more robust feature representations. During inference, dropout is turned off, and the network uses all connections scaled appropriately.\n",
    "\n",
    "#### Training Dynamics\n",
    "\n",
    "Combining data augmentation, appropriate optimizers, and dropout significantly improves MLP performance. Augmentation increases diversity, dropout reduces overfitting, and optimizers ensure efficient and stable learning. Together, these components help build resilient and high-performing neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d06b8a",
   "metadata": {},
   "source": [
    "### **Source Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9877a0",
   "metadata": {},
   "source": [
    "#### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce202ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f78ec",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10), # random rotation\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)), # random translations\n",
    "    transforms.ToTensor(), # 0-1 torch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalizing pixels (-1,1)\n",
    "])\n",
    "\n",
    "# No augmentation for test set\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1a10c",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8891f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_subset, val_subset = random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f45b4",
   "metadata": {},
   "source": [
    "### MLP Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a569d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden1=256, hidden2=128, dropout=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden2, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918c876",
   "metadata": {},
   "source": [
    "#### Training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc31cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = total_correct = 0\n",
    "\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (output.argmax(1) == target).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), total_correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (output.argmax(1) == target).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), total_correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e9725",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d3d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    \"hidden1\": [128, 256],\n",
    "    \"hidden2\": [64, 128],\n",
    "    \"dropout\": [0.2, 0.3],\n",
    "    \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n",
    "    \"lr\": [0.001, 0.0005]\n",
    "}\n",
    "\n",
    "def create_optimizer(name, params, lr):\n",
    "    if name == \"adam\": return optim.Adam(params, lr=lr)\n",
    "    if name == \"sgd\": return optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    if name == \"rmsprop\": return optim.RMSprop(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e1c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: h1=128, h2=64, drop=0.2, opt=adam, lr=0.001\n",
      "Val Acc: 0.8881\n",
      "Testing: h1=128, h2=64, drop=0.2, opt=adam, lr=0.0005\n",
      "Val Acc: 0.8921\n",
      "Testing: h1=128, h2=64, drop=0.2, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7125\n",
      "Testing: h1=128, h2=64, drop=0.2, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6178\n",
      "Testing: h1=128, h2=64, drop=0.2, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8673\n",
      "Testing: h1=128, h2=64, drop=0.2, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8585\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=adam, lr=0.001\n",
      "Val Acc: 0.8806\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=adam, lr=0.0005\n",
      "Val Acc: 0.8759\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7127\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.5810\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8590\n",
      "Testing: h1=128, h2=64, drop=0.3, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8503\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=adam, lr=0.001\n",
      "Val Acc: 0.9019\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=adam, lr=0.0005\n",
      "Val Acc: 0.8992\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7284\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6074\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8284\n",
      "Testing: h1=128, h2=128, drop=0.2, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8783\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=adam, lr=0.001\n",
      "Val Acc: 0.8827\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=adam, lr=0.0005\n",
      "Val Acc: 0.8769\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7228\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6056\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8778\n",
      "Testing: h1=128, h2=128, drop=0.3, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8201\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=adam, lr=0.001\n",
      "Val Acc: 0.9164\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=adam, lr=0.0005\n",
      "Val Acc: 0.9143\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7418\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6079\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8684\n",
      "Testing: h1=256, h2=64, drop=0.2, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8951\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=adam, lr=0.001\n",
      "Val Acc: 0.9068\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=adam, lr=0.0005\n",
      "Val Acc: 0.8972\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=sgd, lr=0.001\n",
      "Val Acc: 0.6994\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6233\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.8778\n",
      "Testing: h1=256, h2=64, drop=0.3, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.9022\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=adam, lr=0.001\n",
      "Val Acc: 0.9269\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=adam, lr=0.0005\n",
      "Val Acc: 0.9218\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7400\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6316\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.9109\n",
      "Testing: h1=256, h2=128, drop=0.2, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8979\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=adam, lr=0.001\n",
      "Val Acc: 0.9126\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=adam, lr=0.0005\n",
      "Val Acc: 0.9096\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=sgd, lr=0.001\n",
      "Val Acc: 0.7256\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=sgd, lr=0.0005\n",
      "Val Acc: 0.6118\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=rmsprop, lr=0.001\n",
      "Val Acc: 0.9029\n",
      "Testing: h1=256, h2=128, drop=0.3, opt=rmsprop, lr=0.0005\n",
      "Val Acc: 0.8744\n",
      "Best Hyperparameters: (256, 128, 0.2, 'adam', 0.001)\n",
      "Best Validation Accuracy: 0.9269\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "for hidden1, hidden2, dropout, opt_name, lr in itertools.product(\n",
    "        param_grid[\"hidden1\"],\n",
    "        param_grid[\"hidden2\"],\n",
    "        param_grid[\"dropout\"],\n",
    "        param_grid[\"optimizer\"],\n",
    "        param_grid[\"lr\"]):\n",
    "\n",
    "    print(f\"Testing: h1={hidden1}, h2={hidden2}, drop={dropout}, opt={opt_name}, lr={lr}\")\n",
    "\n",
    "    model = MLP(hidden1=hidden1, hidden2=hidden2, dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = create_optimizer(opt_name, model.parameters(), lr)\n",
    "\n",
    "    # Train for fewer epochs for tuning\n",
    "    for epoch in range(3):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_params = (hidden1, hidden2, dropout, opt_name, lr)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation Accuracy:\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee9b36",
   "metadata": {},
   "source": [
    "#### Training with best hyperparams and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99739035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=0.6908, Val Acc=0.8681\n",
      "Epoch 2: Train Acc=0.8483, Val Acc=0.9006\n",
      "Epoch 3: Train Acc=0.8731, Val Acc=0.9211\n",
      "Epoch 4: Train Acc=0.8885, Val Acc=0.9350\n",
      "Epoch 5: Train Acc=0.8944, Val Acc=0.9370\n",
      "Epoch 6: Train Acc=0.9019, Val Acc=0.9415\n",
      "Epoch 7: Train Acc=0.9066, Val Acc=0.9397\n",
      "Epoch 8: Train Acc=0.9107, Val Acc=0.9436\n",
      "Epoch 9: Train Acc=0.9130, Val Acc=0.9509\n",
      "Epoch 10: Train Acc=0.9133, Val Acc=0.9511\n"
     ]
    }
   ],
   "source": [
    "h1, h2, drop, opt_name, lr = best_params\n",
    "model = MLP(hidden1=h1, hidden2=h2, dropout=drop).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(opt_name, model.parameters(), lr)\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887622f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(\"Final Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
